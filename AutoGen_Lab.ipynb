{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvTUC0y70MYB"
      },
      "source": [
        "## üõ† Introduction\n",
        "\n",
        "In this notebook, we‚Äôll build a **collaborative multi-agent article generation system** powered by Azure OpenAI and AutoGen‚Äôs agentchat framework, exposed via a minimalist Flask API. Each agent plays a defined editorial role‚Äîtitle generation, content creation, formatting, and format-checking‚Äîworking together to produce a polished Markdown article file.\n",
        "\n",
        "### Core Technologies\n",
        "\n",
        "- **autogen-agentchat** & **autogen_ext[openai,azure]**  \n",
        "  Multi-agent orchestration library for Azure OpenAI models.\n",
        "- **autogen_core**  \n",
        "  Core runtime, message routing, and tool integration.\n",
        "- **Flask**  \n",
        "  Lightweight web framework to expose a `/generate-article` endpoint.\n",
        "- **rich**  \n",
        "  Nicely formatted console output (Markdown rendering in notebook).\n",
        "\n",
        "---\n",
        "\n",
        "## What We‚Äôll Do in This Notebook\n",
        "\n",
        "We divide the pipeline into clear, progressive steps. By the end, you‚Äôll have a running Flask service that, given a topic, spins up a group‚Äêchat among agents to generate, format, verify, and save an article Markdown file.\n",
        "\n",
        "### Step 1: Install Dependencies  \n",
        "\n",
        "```bash\n",
        "pip install -qU autogen-agentchat autogen-ext[openai,azure] flask autogen_core rich\n",
        "```\n",
        "\n",
        "This brings in the AutoGen agentchat extensions, Flask, and Rich for pretty output.\n",
        "\n",
        "### Step 2: Configure Environment & Model Client\n",
        "\n",
        "1. Set Azure OpenAI environment variables:\n",
        "\n",
        "   * `AZURE_OPENAI_DEPLOYMENT_NAME`\n",
        "   * `OPENAI_API_VERSION`\n",
        "   * `AZURE_OPENAI_API_KEY`\n",
        "   * `AZURE_OPENAI_ENDPOINT`\n",
        "\n",
        "2. Initialize an `AzureOpenAIChatCompletionClient` for model calls.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 3: Define the ‚ÄúGenerate Title‚Äù Tool\n",
        "\n",
        "* **Function**: `generate_title(topic: str) ‚Üí str`\n",
        "* Sends a prompt to the model to create a concise, catchy headline.\n",
        "* Registered as a `FunctionTool` so agents can invoke it during content creation.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 4: Load Article Template\n",
        "\n",
        "* Read in `article_template.txt` from disk.\n",
        "* This Markdown skeleton guides the ContentCreator and Formatter agents.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 5: Build Base Agent Classes\n",
        "\n",
        "* **BaseGroupChatAgent**\n",
        "\n",
        "  * Wraps common chat history management, message handlers, and console output.\n",
        "  * Routes two message types:\n",
        "\n",
        "    * `GroupChatMessage` for plain user/model content.\n",
        "    * `RequestToSpeak` for turn-taking.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 6: Implement Specialized Agents\n",
        "\n",
        "1. **ContentCreatorAgent**\n",
        "\n",
        "   * Persona: ‚ÄúContent Creator‚Äù\n",
        "   * First invokes the `generate_title` tool, then writes the article into the provided template.\n",
        "2. **FormatterAgent**\n",
        "\n",
        "   * Persona: ‚ÄúFormatter‚Äù\n",
        "   * Rewrites raw content to adhere precisely to the Markdown template.\n",
        "3. **FormatCheckerAgent**\n",
        "\n",
        "   * Persona: ‚ÄúFormat Checker‚Äù\n",
        "   * Validates template compliance; responds with `APPROVED` or suggestions.\n",
        "\n",
        "Each agent extends `BaseGroupChatAgent`, supplies its own `system_message`, and (for ContentCreator) wires in the title-generation tool.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 7: Orchestrate with GroupChatManager\n",
        "\n",
        "* Tracks conversation history and enforces save logic when ‚ÄúAPPROVED‚Äù is received.\n",
        "* Decides which agent speaks next by prompting the model with:\n",
        "\n",
        "  * Available roles\n",
        "  * Conversation transcript\n",
        "  * Instructions to respond with exactly one role name.\n",
        "* On approval, writes the final Markdown file (`<topic>.md`) and stops the runtime.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 8: Expose as a Flask Endpoint\n",
        "\n",
        "* **`POST /generate-article?topic=‚Ä¶`**\n",
        "\n",
        "  1. Spawns a background thread running `_run_group_chat(topic)`.\n",
        "  2. Blocks until the event signals completion.\n",
        "  3. Returns a JSON response with the filename of the generated article.\n",
        "\n",
        "Under the hood, `_run_group_chat`:\n",
        "\n",
        "1. Instantiates a `SingleThreadedAgentRuntime`.\n",
        "2. Registers all agents and their subscriptions on the `\"group_chat\"` topic.\n",
        "3. Publishes the initial UserMessage:\n",
        "\n",
        "   > ‚ÄúWrite a professional article on ‚Äò<topic>‚Äô using the given template.‚Äù\n",
        "4. Awaits idle, then closes the model client.\n",
        "\n",
        "---\n",
        "\n",
        "## By the End of This Notebook\n",
        "\n",
        "* A **Flask-powered API** that, given any topic, generates a full Markdown article.\n",
        "* A **multi-agent workflow** illustrating:\n",
        "\n",
        "  * Tool invocation within agentchat,\n",
        "  * Turn-taking logic,\n",
        "  * Role-based system prompts,\n",
        "  * Automated title generation, formatting, and quality checks.\n",
        "* A saved file `\"<topic>.md\"` containing your approved article.\n",
        "\n",
        "This pipeline showcases how to combine **Azure OpenAI**, **AutoGen agentchat**, and a simple **web interface** to automate end-to-end content creation tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MF9KxF9JBHu"
      },
      "source": [
        "## Step 1: Install Dependencies\n",
        "\n",
        "Before running any code, we need to install all required Python packages. This single combined command:\n",
        "\n",
        "* **Sets up the AutoGen agentchat framework** (`autogen-agentchat`) and **Azure/OpenAI extensions** (`autogen-ext[openai,azure]`).\n",
        "* **Installs Flask** for providing an HTTP API, the **AutoGen core** runtime, and **Rich** for beautifully formatted console logs.\n",
        "\n",
        "By using `-qU`, we ensure quiet output and upgrade to the latest versions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08rygCn8mfvw"
      },
      "outputs": [],
      "source": [
        "# Install AutoGen agent framework with OpenAI and Azure support\n",
        "!pip install -qU \"autogen-agentchat\" \"autogen-ext[openai,azure]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4Ovk0qymn90"
      },
      "outputs": [],
      "source": [
        "# Install Flask (API server), AutoGen core, and Rich (for better console output)\n",
        "!pip install -qU flask autogen_core rich"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmVdoxkWDaeF"
      },
      "source": [
        "## Step 2: Import Libraries & Configure Model Client\n",
        "\n",
        "In this cell, we:\n",
        "\n",
        "1. **Import standard libraries** for async execution (`asyncio`), threading (`Thread`), JSON handling, UUID generation, and URL encoding.\n",
        "2. **Bring in Jupyter utilities** (`display`, `ipywidgets`) for potential interactive input.\n",
        "3. **Load Flask** for the web endpoint.\n",
        "4. **Define Pydantic models** and **AutoGen core classes** for message routing and agent orchestration.\n",
        "5. **Configure Azure OpenAI credentials** via environment variables. Replace placeholders with your actual key and endpoint.\n",
        "6. **Instantiate** an `AzureOpenAIChatCompletionClient` to handle all subsequent LLM calls.\n",
        "\n",
        "This setup cell forms the backbone of our multi-agent system, ensuring all dependencies and credentials are initialized before any logic runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LT7q0bxZmpmY"
      },
      "outputs": [],
      "source": [
        "# Standard libraries for OS, JSON, UUIDs, async, threading, and typing\n",
        "import os\n",
        "import json\n",
        "import uuid\n",
        "import asyncio\n",
        "import urllib.parse\n",
        "from threading import Thread\n",
        "from typing import List\n",
        "\n",
        "\n",
        "# Flask for API creation\n",
        "from flask import Flask, request, jsonify\n",
        "\n",
        "# Typing extensions and data validation\n",
        "from typing_extensions import Annotated\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# AutoGen core components for agent setup and messaging\n",
        "import autogen_core\n",
        "from autogen_core import (\n",
        "    DefaultTopicId, MessageContext, SingleThreadedAgentRuntime, TopicId,\n",
        "    TypeSubscription, message_handler, RoutedAgent, CancellationToken\n",
        ")\n",
        "\n",
        "# Message and model handling in AutoGen\n",
        "from autogen_core.models import (\n",
        "    SystemMessage, UserMessage, AssistantMessage, ChatCompletionClient,\n",
        "    LLMMessage, FunctionExecutionResult, FunctionExecutionResultMessage\n",
        ")\n",
        "\n",
        "# Tool wrapper and Azure OpenAI client\n",
        "from autogen_core.tools import FunctionTool\n",
        "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
        "\n",
        "# Rich output for CLI rendering\n",
        "from rich.console import Console\n",
        "from rich.markdown import Markdown\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bd8pzHNIm8wk"
      },
      "outputs": [],
      "source": [
        "# Set environment variables for Azure OpenAI configurationos.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"] = \"gpt-4o-mini\"\n",
        "os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"] = \"gpt-4o-mini\"          # Azure model deployment name\n",
        "os.environ[\"OPENAI_API_VERSION\"] = \"2024-12-01-preview\"             # API version for Azure OpenAI\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"6qZqBQ5RB7ImYsP7ajBcIX7rZUnD22vTz76QI5R3FWlWAJ0EjvcXJQQJ99BEACHYHv6XJ3w3AAAAACOGdXrz\"                             # Your Azure OpenAI API key (fill this)\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://ai-hpesgalite4096ai784823033224.openai.azure.com\"                            # Azure OpenAI endpoint URL (fill this)\n",
        "\n",
        "# Initialize the Azure OpenAI chat model client\n",
        "model_client = AzureOpenAIChatCompletionClient(\n",
        "    model=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],              # Deployment name\n",
        "    api_base=os.environ[\"AZURE_OPENAI_ENDPOINT\"],                  # Endpoint URL\n",
        "    api_type=\"azure\",                                              # API type\n",
        "    api_version=os.environ[\"OPENAI_API_VERSION\"],                  # API version\n",
        "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"]                     # API key\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7HMQqdUJn6L"
      },
      "source": [
        "## Step 3: Initialize Flask Application\n",
        "\n",
        "Here we create the Flask application instance. It will later host our `/generate-article` endpoint, receiving HTTP POST requests to trigger the article generation pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XI5XbhL2m9Pv"
      },
      "outputs": [],
      "source": [
        "# Initialize the Flask web application\n",
        "app = Flask(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzgmuXD5Dgu0"
      },
      "source": [
        "## Step 4: Define Title-Generation Tool\n",
        "\n",
        "We build a reusable asynchronous function `generate_title`:\n",
        "\n",
        "* **Input**: A plain string topic.\n",
        "* **Action**: Crafts a prompt instructing the model to generate a clean, catchphrase-style headline.\n",
        "* **Output**: Returns the generated title text.\n",
        "\n",
        "This function is then wrapped in `FunctionTool`, which allows agents to invoke it as a first-class ‚Äútool‚Äù within the chat flow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1aaE0pP8-sG"
      },
      "outputs": [],
      "source": [
        "# Async function to generate an article title using LLM\n",
        "async def generate_title(topic: Annotated[str, \"The user-provided article topic\"]) -> str:\n",
        "    prompt = f\"\"\"You are a headline expert. Generate a professional, concise, and catchy article title for the topic below.\n",
        "\n",
        "    Topic: {topic}\n",
        "\n",
        "    Only return the title text. Do not include quotes, labels, or extra commentary.\"\"\"\n",
        "\n",
        "    # Call the LLM to get a title based on the prompt\n",
        "    completion = await model_client.create(\n",
        "        messages=[UserMessage(content=prompt, source=\"user\")]\n",
        "    )\n",
        "    return completion.content.strip()  # üßπ Return clean title text\n",
        "\n",
        "# üõ† Register the function as a tool for use in AutoGen workflows\n",
        "generate_title_tool = FunctionTool(\n",
        "    generate_title,\n",
        "    description=\"Use LLM to generate a professional article title based on the input topic.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrg2yp61DoFz"
      },
      "source": [
        "## Step 5: Load Article Template & Setup Console\n",
        "\n",
        "We read our predefined Markdown skeleton (`article_template.txt`), which defines headings and placeholders for the final article. Simultaneously, we tear up Rich‚Äôs `Console` for live Markdown rendering as the pipeline runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Py0PkpOlnKa1"
      },
      "outputs": [],
      "source": [
        "# Load the article template from file\n",
        "with open(\"article_template.txt\", \"r\") as f:\n",
        "    article_template = f.read()\n",
        "\n",
        "# Initialize Rich console for pretty terminal output\n",
        "console = Console()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jN8Rr2vbD0t1"
      },
      "source": [
        "## Step 6: Define Message Schemas\n",
        "\n",
        "Pydantic models help enforce structure for messages exchanged between agents:\n",
        "\n",
        "* **GroupChatMessage**: Wraps a `UserMessage` body when broadcasting agent content.\n",
        "* **RequestToSpeak**: A simple signal type indicating it's an agent‚Äôs turn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YIpiPvPnU8a"
      },
      "outputs": [],
      "source": [
        "# Define a message wrapper for group chat communication\n",
        "class GroupChatMessage(BaseModel):\n",
        "    body: UserMessage  # Holds a user-generated message\n",
        "\n",
        "# Define a signal model to request speaking in group chat\n",
        "class RequestToSpeak(BaseModel):\n",
        "    pass  # Used as a trigger with no additional data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVFmum94J533"
      },
      "source": [
        "## Step 7: Build Base Agent Class\n",
        "\n",
        "`BaseGroupChatAgent` centralizes shared behaviors:\n",
        "\n",
        "1. **Initialization**: Accepts a role description, topic type, LLM client, and persona system message.\n",
        "2. **Message Handling**: On receiving a chat message, it appends both a system-transfer note and the content to its history.\n",
        "3. **Speaking**: When prompted to speak, it prints its header, sends the full history (including its system prompt) to the LLM, records the response, and broadcasts it back to the group.\n",
        "\n",
        "This scaffolding underpins all specialized agents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zl6fAGe4nXRf"
      },
      "outputs": [],
      "source": [
        "# Define a base group chat agent using AutoGen's RoutedAgent\n",
        "class BaseGroupChatAgent(RoutedAgent):\n",
        "    def __init__(self, description: str, group_chat_topic_type: str, model_client: ChatCompletionClient, system_message: str) -> None:\n",
        "        super().__init__(description=description)\n",
        "        self._group_chat_topic_type = group_chat_topic_type  # Group topic category/type\n",
        "        self._model_client = model_client                    # Azure/OpenAI model client\n",
        "        self._system_message = SystemMessage(content=system_message)  # Initial persona setup\n",
        "        self._chat_history: List[LLMMessage] = []            # Track conversation history\n",
        "\n",
        "    # Handle incoming user messages routed to this agent\n",
        "    @message_handler\n",
        "    async def handle_message(self, message: GroupChatMessage, ctx: MessageContext) -> None:\n",
        "        self._chat_history.extend([\n",
        "            UserMessage(content=f\"Transferred to {message.body.source}\", source=\"system\"),  # System note\n",
        "            message.body  # Add the actual user message\n",
        "        ])\n",
        "\n",
        "    # Handle a request to speak event (e.g., when the agent is chosen to respond)\n",
        "    @message_handler\n",
        "    async def handle_request_to_speak(self, message: RequestToSpeak, ctx: MessageContext) -> None:\n",
        "        console.print(Markdown(f\"### {self.id.type}: \"))  # Print who is speaking\n",
        "        # Add a system instruction to assume persona\n",
        "        self._chat_history.append(UserMessage(content=f\"Transferred to {self.id.type}, adopt the persona immediately.\", source=\"system\"))\n",
        "\n",
        "        # Query the model with system message + history\n",
        "        completion = await self._model_client.create([self._system_message] + self._chat_history)\n",
        "        assert isinstance(completion.content, str)\n",
        "\n",
        "        # Store and display assistant's response\n",
        "        self._chat_history.append(AssistantMessage(content=completion.content, source=self.id.type))\n",
        "        console.print(Markdown(completion.content))\n",
        "\n",
        "        # Publish the assistant's message to the group chat\n",
        "        await self.publish_message(\n",
        "            GroupChatMessage(body=UserMessage(content=completion.content, source=self.id.type)),\n",
        "            topic_id=DefaultTopicId(type=self._group_chat_topic_type)\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNcVTRfVD6uW"
      },
      "source": [
        "## Step 8: Implement Specialized Agents\n",
        "\n",
        "We extend `BaseGroupChatAgent` to define three distinct roles:\n",
        "\n",
        "1. **ContentCreatorAgent**:\n",
        "\n",
        "   * Uses `generate_title_tool` to create the headline.\n",
        "   * Writes the full article within the provided Markdown structure.\n",
        "2. **FormatterAgent**:\n",
        "\n",
        "   * Reprocesses raw content to strictly conform to the template‚Äôs layout.\n",
        "3. **FormatCheckerAgent**:\n",
        "\n",
        "   * Reviews final output, returning `APPROVED` or specific formatting suggestions.\n",
        "\n",
        "Each agent sets its own system prompt and tools, then leverages the shared `handle_request_to_speak` flow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_cGgJad7dli"
      },
      "outputs": [],
      "source": [
        "# Agent responsible for generating article title and content\n",
        "class ContentCreatorAgent(BaseGroupChatAgent):\n",
        "    def __init__(self, group_chat_topic_type: str, model_client: ChatCompletionClient):\n",
        "        # Initialize with a system message and markdown template instruction\n",
        "        super().__init__(\n",
        "            \"Content Creator\",  # Agent description\n",
        "            group_chat_topic_type,\n",
        "            model_client,\n",
        "            f\"\"\"You are a content creator. First, use the `generate_title` tool to come up with a professional article title. Then write a full article using the provided markdown structure:\\n{article_template}\"\"\"\n",
        "        )\n",
        "        self._tools = [generate_title_tool]  # Register the title generation tool\n",
        "\n",
        "    # üéôÔ∏è Respond when asked to speak in the group chat\n",
        "    @message_handler\n",
        "    async def handle_request_to_speak(self, message: RequestToSpeak, ctx: MessageContext) -> None:\n",
        "        console.print(Markdown(f\"### {self.id.type}: \"))  # Print agent role visibly\n",
        "        self._chat_history.append(\n",
        "            UserMessage(content=\"Generate the article starting with title generation.\", source=\"system\")\n",
        "        )\n",
        "\n",
        "        # üîÑ First attempt: let LLM decide to generate or invoke a tool\n",
        "        completion = await self._model_client.create(\n",
        "            messages=[self._system_message] + self._chat_history,\n",
        "            tools=self._tools,\n",
        "            cancellation_token=ctx.cancellation_token\n",
        "        )\n",
        "\n",
        "        if isinstance(completion.content, str):\n",
        "            # üßæ LLM directly responded with article content\n",
        "            self._chat_history.append(AssistantMessage(content=completion.content, source=self.id.type))\n",
        "        else:\n",
        "            # üîß Tool call path: LLM invoked the title generation tool\n",
        "            tool_call = completion.content[0]\n",
        "            arguments = json.loads(tool_call.arguments)\n",
        "            tool_result = await self._tools[0].run_json(arguments, ctx.cancellation_token)\n",
        "\n",
        "            print(f\"üè∑Ô∏è Generated Title: {tool_result.strip()}\")\n",
        "\n",
        "            # Add the tool call and result to the chat history\n",
        "            self._chat_history.append(\n",
        "                AssistantMessage(content=[tool_call], source=self.id.type)\n",
        "            )\n",
        "            self._chat_history.append(\n",
        "                FunctionExecutionResultMessage(content=[\n",
        "                    FunctionExecutionResult(\n",
        "                        call_id=tool_call.id,\n",
        "                        content=tool_result,\n",
        "                        is_error=False,\n",
        "                        name=self._tools[0].name\n",
        "                    )\n",
        "                ])\n",
        "            )\n",
        "\n",
        "            # Re-run LLM after tool execution to generate article body\n",
        "            completion = await self._model_client.create(\n",
        "                messages=[self._system_message] + self._chat_history,\n",
        "                cancellation_token=ctx.cancellation_token\n",
        "            )\n",
        "            self._chat_history.append(AssistantMessage(content=completion.content, source=self.id.type))\n",
        "\n",
        "        # Display the final article content\n",
        "        console.print(Markdown(self._chat_history[-1].content))\n",
        "\n",
        "        # Broadcast the final message to the group chat\n",
        "        await self.publish_message(\n",
        "            GroupChatMessage(body=UserMessage(content=self._chat_history[-1].content, source=self.id.type)),\n",
        "            topic_id=DefaultTopicId(type=self._group_chat_topic_type)\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wm4kkAFrnY-Y"
      },
      "outputs": [],
      "source": [
        "# Agent to format content using the predefined article markdown template\n",
        "class FormatterAgent(BaseGroupChatAgent):\n",
        "    def __init__(self, group_chat_topic_type: str, model_client: ChatCompletionClient):\n",
        "        super().__init__(\n",
        "            \"Formatter\",  # Agent role/description\n",
        "            group_chat_topic_type,\n",
        "            model_client,\n",
        "            f\"You are a formatter. Format the content into the expected markdown layout using the following template:\\n{article_template}\"\n",
        "        )\n",
        "\n",
        "# Agent to check if the article formatting follows the expected markdown structure\n",
        "class FormatCheckerAgent(BaseGroupChatAgent):\n",
        "    def __init__(self, group_chat_topic_type: str, model_client: ChatCompletionClient):\n",
        "        super().__init__(\n",
        "            \"Format Checker\",  # Agent role/description\n",
        "            group_chat_topic_type,\n",
        "            model_client,\n",
        "            \"You are a format checker. Verify if the article is correctly formatted as per the template. Respond only with APPROVED or specific SUGGESTIONS.\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EVUe8AqD_ic"
      },
      "source": [
        "## Step 9: Orchestrate with GroupChatManager\n",
        "\n",
        "`GroupChatManager` dictates which agent speaks next and handles finalization:\n",
        "\n",
        "* **Conversation Tracking**: Maintains a full transcript.\n",
        "* **Save Logic**: Detects `APPROVED` and writes the complete article to `<topic>.md`.\n",
        "* **Turn-Taking**: Prompts the LLM to choose the next role from the remaining participants.\n",
        "\n",
        "This central controller ensures smooth collaboration and triggers shutdown upon completion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUxIQ2xtnaVG"
      },
      "outputs": [],
      "source": [
        "# GroupChatManager orchestrates the agent conversation flow\n",
        "class GroupChatManager(RoutedAgent):\n",
        "    def __init__(self, participant_topic_types: List[str], model_client: ChatCompletionClient, participant_descriptions: List[str], topic: str) -> None:\n",
        "        super().__init__(\"Group Chat Manager\")  # Agent name\n",
        "        self._participant_topic_types = participant_topic_types        # List of agent types (e.g., ContentCreator, Formatter)\n",
        "        self._model_client = model_client                              # LLM client\n",
        "        self._chat_history: List[UserMessage] = []                     # Tracks user messages in the chat\n",
        "        self._participant_descriptions = participant_descriptions      # Descriptions for each agent\n",
        "        self._previous_participant_topic_type: str | None = None       # Prevent immediate repetition\n",
        "        self._topic = topic                                            # Topic of the article\n",
        "        self._last_complete_article = \"\"                               # Stores article before approval\n",
        "\n",
        "    # Handles incoming messages from agents\n",
        "    @message_handler\n",
        "    async def handle_message(self, message: GroupChatMessage, ctx: MessageContext) -> None:\n",
        "        self._chat_history.append(message.body)\n",
        "\n",
        "        if isinstance(message.body.content, str):\n",
        "            content = message.body.content.strip()\n",
        "            print(f\"üîπ Received content from {message.body.source}: {content[:60]}...\")\n",
        "\n",
        "            # Store completed article before final approval\n",
        "            if content.endswith(\"End of the article.\"):\n",
        "                self._last_complete_article = content\n",
        "                print(\"‚úÖ Stored complete article content in memory.\")\n",
        "\n",
        "            # If format checker approves, save the article to file\n",
        "            elif content == \"APPROVED\" and self._last_complete_article:\n",
        "                base_filename = self._topic.strip() or \"article\"\n",
        "                with open(f\"{base_filename}.md\", \"w\") as f:\n",
        "                    f.write(self._last_complete_article)\n",
        "                print(f\"üìÑ Saved approved article to '{base_filename}.md'.\")\n",
        "\n",
        "                # üîö Stop runtime gracefully (if set)\n",
        "                if self._runtime:\n",
        "                    print(\"üõë Stopping runtime after approval.\")\n",
        "                    # self._runtime.request_termination()  # Optional: explicitly stop if runtime support exists\n",
        "                else:\n",
        "                    print(\"‚ö† Runtime reference is not set; cannot stop runtime.\")\n",
        "                return\n",
        "\n",
        "        # Decide next speaking agent based on chat history\n",
        "        history = \"\\n\".join([f\"{m.source}: {m.content}\" for m in self._chat_history])\n",
        "        roles = \"\\n\".join([\n",
        "            f\"{t}: {d}\" for t, d in zip(self._participant_topic_types, self._participant_descriptions)\n",
        "            if t != self._previous_participant_topic_type  # Avoid back-to-back same agent\n",
        "        ])\n",
        "\n",
        "        prompt = (\n",
        "            \"You are managing a collaborative article generation task.\\n\"\n",
        "            f\"{roles}\\n\\nConversation:\\n{history}\\n\\n\"\n",
        "            \"Select the next role to speak (respond ONLY with one of: ContentCreator, Formatter, FormatChecker).\"\n",
        "        )\n",
        "\n",
        "        system_message = SystemMessage(content=prompt)\n",
        "\n",
        "        # Ask model to choose next agent\n",
        "        completion = await self._model_client.create([system_message], cancellation_token=ctx.cancellation_token)\n",
        "        response = completion.content.strip()\n",
        "        print(f\"üîç Model suggested next role: '{response}'\")\n",
        "\n",
        "        # Validate and trigger the next speaker\n",
        "        selected_topic_type = next(\n",
        "            (t for t in self._participant_topic_types if t.lower() == response.lower()), None\n",
        "        )\n",
        "\n",
        "        if selected_topic_type:\n",
        "            self._previous_participant_topic_type = selected_topic_type\n",
        "            await self.publish_message(RequestToSpeak(), DefaultTopicId(type=selected_topic_type))\n",
        "            print(f\"‚û° Published RequestToSpeak to: {selected_topic_type}\")\n",
        "        else:\n",
        "            print(f\"‚ùå Invalid role selected by model: '{response}' ‚Äî skipping this round.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2VKlzeqEKkq"
      },
      "source": [
        "## Step 10: Expose as Flask Endpoint\n",
        "\n",
        "We define `/generate-article` that:\n",
        "\n",
        "1. **Spawns** a background thread calling `_run_group_chat(topic)`.\n",
        "2. **Blocks** until the thread signals completion via an event.\n",
        "3. **Returns** JSON containing the output filename.\n",
        "\n",
        "The helper `_run_group_chat`:\n",
        "\n",
        "* Registers all agents and subscriptions,\n",
        "* Publishes the initial user request,\n",
        "* Waits for the runtime to idle,\n",
        "* Closes the LLM client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVIDkTa2ncSf"
      },
      "outputs": [],
      "source": [
        "# üì° Flask route to trigger article generation via HTTP POST\n",
        "@app.route(\"/generate-article\", methods=[\"POST\"])\n",
        "def generate_article():\n",
        "    # Extract the article topic from query params or fallback to default\n",
        "    topic = request.args.get(\"topic\") or request.args.get(\"T\") or \"Benefit of AI in Healthcare\"\n",
        "    topic = topic.strip()\n",
        "\n",
        "    # Event to block response until background generation finishes\n",
        "    import threading\n",
        "    done_event = threading.Event()\n",
        "\n",
        "    # Launch group chat in a background thread\n",
        "    def run_generation():\n",
        "        asyncio.run(_run_group_chat(topic))  # Run async function inside thread\n",
        "        done_event.set()  # Notify completion\n",
        "\n",
        "    thread = Thread(target=run_generation)\n",
        "    thread.start()\n",
        "\n",
        "    # ‚è±Wait until the generation thread completes\n",
        "    done_event.wait()\n",
        "\n",
        "    filename = f\"{topic}.md\"\n",
        "    return jsonify({\n",
        "        \"status\": \"done\",\n",
        "        \"message\": \"Article generated\",\n",
        "        \"file\": filename,\n",
        "        \"topic\": topic\n",
        "    })\n",
        "\n",
        "# Asynchronous orchestration of agents for article generation\n",
        "async def _run_group_chat(topic):\n",
        "    runtime = SingleThreadedAgentRuntime()  # Single-threaded AutoGen runtime\n",
        "    participants = [\"ContentCreator\", \"Formatter\", \"FormatChecker\"]  # Agent types\n",
        "    descriptions = [\n",
        "        \"Creates initial article content.\",\n",
        "        \"Formats the article.\",\n",
        "        \"Checks article formatting.\"\n",
        "    ]\n",
        "\n",
        "    # Register each agent in the runtime\n",
        "    await ContentCreatorAgent.register(runtime, \"ContentCreator\", lambda: ContentCreatorAgent(\"group_chat\", model_client))\n",
        "    await FormatterAgent.register(runtime, \"Formatter\", lambda: FormatterAgent(\"group_chat\", model_client))\n",
        "    await FormatCheckerAgent.register(runtime, \"FormatChecker\", lambda: FormatCheckerAgent(\"group_chat\", model_client))\n",
        "    await GroupChatManager.register(runtime, \"GroupChatManager\", lambda: GroupChatManager(participants, model_client, descriptions, topic))\n",
        "\n",
        "    # Subscribe each agent to relevant topic types for message routing\n",
        "    for t in participants:\n",
        "        await runtime.add_subscription(TypeSubscription(topic_type=t, agent_type=t))           # Direct topic\n",
        "        await runtime.add_subscription(TypeSubscription(topic_type=\"group_chat\", agent_type=t)) # Shared group chat\n",
        "    await runtime.add_subscription(TypeSubscription(topic_type=\"group_chat\", agent_type=\"GroupChatManager\"))\n",
        "\n",
        "    runtime.start()  # Start message loop\n",
        "    session_id = str(uuid.uuid4())  # Generate unique session\n",
        "\n",
        "    # üì® Trigger initial article generation instruction\n",
        "    await runtime.publish_message(\n",
        "        GroupChatMessage(\n",
        "            body=UserMessage(\n",
        "                content=f\"Write a professional article on '{topic}' using the given template.\",\n",
        "                source=\"User\",\n",
        "            )\n",
        "        ),\n",
        "        TopicId(type=\"group_chat\", source=session_id),\n",
        "    )\n",
        "\n",
        "    await runtime.stop_when_idle()  # Stop when all agents are idle\n",
        "    await model_client.close()      # Clean up model client connection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VpU_72KKSSZ"
      },
      "source": [
        "## Step 11: Run Flask Server & Test\n",
        "\n",
        "Finally, we launch Flask in a daemon thread and trigger the pipeline via `curl`. The notebook prints each step‚Äôs output and any errors, concluding with the saved Markdown filename."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pZmdhwsnzFT"
      },
      "outputs": [],
      "source": [
        "# Function to launch Flask server\n",
        "def run_flask():\n",
        "    print(\"Starting Flask server at http://localhost:5001 ...\")\n",
        "    app.run(port=5001, debug=False, use_reloader=False)  # Start Flask app on port 5001\n",
        "\n",
        "# Run Flask server in a background daemon thread\n",
        "flask_thread = Thread(target=run_flask)\n",
        "flask_thread.daemon = True  # Ensure thread exits when main program ends\n",
        "flask_thread.start()        # Start the server thread\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcMY4Z0PnwGn"
      },
      "outputs": [],
      "source": [
        "# Prompt user to enter an article topic\n",
        "article_topic = input(\"Enter the topic of the article: \").strip()\n",
        "\n",
        "# URL-encode the topic to safely pass it as a query parameter\n",
        "encoded_topic = urllib.parse.quote(article_topic)\n",
        "\n",
        "# Make a POST request to the Flask endpoint to trigger article generation\n",
        "!curl -X POST \"http://localhost:5001/generate-article?topic={encoded_topic}\" -H \"Content-Type: application/json\"\n",
        "\n",
        "# Inform the user that the article has been saved\n",
        "print(f\"\\n‚úÖ Article has been saved as '{article_topic}.md'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 12 : Deployment\n",
        "\n",
        "We will now package this application as a Docker container to make it ready for deployment to any service of our choice.\n",
        "\n",
        "PLEASE NOTE - You can skip the following section if you are using Google Colab, the following section requires a virutal machine(VM)/workstation with Docker setup complete in order to proceed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we will create the Dockerfile to package and make the application deployment ready. We have already prepared the Dockerfile for you so you do not need to run the cell below.\n",
        "At this point, close this notebook and open a Terminal window in your JupyterLab environment and follow the steps below - \n",
        "1. Enter the following command to run our Docker container\n",
        "```bash\n",
        "    docker build -t .\n",
        "    docker run -p 5002:5002 -v $(pwd)/output:/app/output autogen-agent\n",
        "```\n",
        "2. Now open another terminal window and send a cURL request to our exposed endpoint\n",
        "```bash\n",
        "    curl -X POST \"http://localhost:5002/generate-article?topic=AI%20in%20Space\" -H \"Content-Type: application/json\"\n",
        "```\n",
        "Given below is the Dockerfile we are using, you do not need to execute this cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use official Python runtime base image\n",
        "FROM python:3.11-slim\n",
        "\n",
        "# Set environment variables\n",
        "ENV PYTHONDONTWRITEBYTECODE=1\n",
        "ENV PYTHONUNBUFFERED=1\n",
        "\n",
        "# Set working directory in the container\n",
        "WORKDIR /app\n",
        "\n",
        "# Copy requirements and install dependencies\n",
        "COPY requirements.txt .\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# Copy app source code\n",
        "COPY output .\n",
        "COPY app.py .\n",
        "COPY article_template.txt .\n",
        "\n",
        "# Expose the port Flask runs on\n",
        "EXPOSE 5002\n",
        "\n",
        "# Run the app\n",
        "CMD [\"python\", \"app.py\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nPt01WKKiWM"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates how to build a fully automated, multi-agent article generator using Azure OpenAI and AutoGen‚Äôs **agentchat** framework, exposed via a simple Flask API. The workflow consists of:\n",
        "\n",
        "1. **Environment Setup**  \n",
        "   Install & import all necessary packages (AutoGen agentchat, Azure/OpenAI extensions, Flask, core runtime, Rich).\n",
        "\n",
        "2. **Model Configuration**  \n",
        "   Set Azure OpenAI credentials and instantiate a chat completion client for interacting with the LLM.\n",
        "\n",
        "3. **Tool Definition**  \n",
        "   Create an asynchronous `generate_title` function wrapped as a `FunctionTool` to produce professional article headlines.\n",
        "\n",
        "4. **Template & Messaging**  \n",
        "   Load a Markdown article template and define Pydantic schemas for structured inter-agent messaging (`GroupChatMessage`, `RequestToSpeak`).\n",
        "\n",
        "5. **Agent Infrastructure**  \n",
        "   Implement `BaseGroupChatAgent` to handle message routing, history tracking, and LLM calls; extend it to three specialized agents:\n",
        "   - **ContentCreatorAgent**: Generates title and article body.\n",
        "   - **FormatterAgent**: Formats content to match the template.\n",
        "   - **FormatCheckerAgent**: Validates final formatting.\n",
        "\n",
        "6. **Orchestration**  \n",
        "   Use `GroupChatManager` to manage turn-taking, collect the full transcript, detect approval, save the final Markdown file, and stop the runtime.\n",
        "\n",
        "7. **API Exposure & Execution**  \n",
        "   Define a Flask endpoint `/generate-article` that spawns the agent runtime in a background thread, blocks until completion, and returns the generated filename. Finally, launch the server and test the full pipeline via a `curl` request, resulting in a saved `<topic>.md` file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwbbvbsmL3Nt"
      },
      "source": [
        "## Lessons Learned & Future Applications\n",
        "\n",
        "- **Clear Role Separation**  \n",
        "  Defining distinct agent personas (title‚Äêcreator, content‚Äêwriter, formatter, checker) ensures each component has a single responsibility, simplifying development and testing.\n",
        "\n",
        "- **Tool-Based LLM Integration**  \n",
        "  Wrapping LLM prompts as `FunctionTool` objects provides a structured, reusable interface for agents to invoke model capabilities safely.\n",
        "\n",
        "- **Flexible Orchestration Logic**  \n",
        "  The `GroupChatManager` pattern‚Äîusing transcripts and LLM prompts to decide turn-taking‚Äîcan adapt to any multi-step workflow requiring dynamic hand-offs.\n",
        "\n",
        "- **Async + Web Server Synergy**  \n",
        "  Combining an asynchronous agent runtime with a Flask endpoint and background threads shows how to expose conversational pipelines as easy-to-call HTTP services.\n",
        "\n",
        "### Possible Extensions\n",
        "\n",
        "- **Automated Data Reporting**  \n",
        "  Agents for data ingestion, chart creation, narrative writing, and format validation can generate end-of-period business or research reports automatically.\n",
        "\n",
        "- **Support Ticket Handling**  \n",
        "  Chain agents for intent classification, response drafting, sentiment analysis, and escalation, delivering a fully automated support workflow.\n",
        "\n",
        "- **Code Generation & Review**  \n",
        "  Implement agents for snippet generation, linting, test creation, and compliance checks to streamline and automate parts of the software development lifecycle.\n",
        "\n",
        "By applying this modular, tool-oriented, orchestrated pattern, you can rapidly build domain-specific multi-agent pipelines that leverage LLMs for a wide variety of tasks.  \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
